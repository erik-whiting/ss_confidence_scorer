{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39227d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_quality_model.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a382ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN = (64, 32)           # MLP sizes\n",
    "VAL_FRACTION = 0.15\n",
    "TEST_FRACTION = 0.15        # remaining is train\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "EASY_CSV = \"chem_map_all_easy_preds_enriched.csv\"\n",
    "HARD_CSV = \"chem_map_all_hard_preds_enriched.csv\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"sequence_length\",\"gc_content\",\"sequence_entropy\",\"mfe\",\"ens_def\",\n",
    "    \"longest_sequential_A\",\"longest_sequential_C\",\"longest_sequential_U\",\"longest_sequential_G\",\n",
    "    \"longest_GC_helix\",\"GU_pairs\",\"rate_of_bps_predicted\",\n",
    "    \"hairpin_count\",\"junction_count\",\"helix_count\",\"singlestrand_count\",\n",
    "    \"mway_junction_count\",\"AU_pairs_in_helix_terminal_ends\",\n",
    "    \"helices_with_reverse_complement\",\"hairpins_with_gt4_unpaired_nts\",\n",
    "]\n",
    "\n",
    "BUNDLE_PATH = \"quality_model_bundle.joblib\"  # everything saved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe247c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Repro\n",
    "# ---------------------------\n",
    "def set_seed(seed: int = SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data\n",
    "# ---------------------------\n",
    "def load_and_prepare() -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    easy = pd.read_csv(EASY_CSV)\n",
    "    hard = pd.read_csv(HARD_CSV)\n",
    "    easy = easy.assign(target=1)\n",
    "    hard = hard.assign(target=0)\n",
    "\n",
    "    df = pd.concat([easy, hard], ignore_index=True)\n",
    "\n",
    "    # Keep only rows with all required features present\n",
    "    X = df[FEATURES].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    mask = X.notna().all(axis=1)\n",
    "    df = df.loc[mask].reset_index(drop=True)\n",
    "    X = df[FEATURES].astype(float).to_numpy(dtype=np.float32)\n",
    "    y = df[\"target\"].to_numpy(dtype=np.float32)\n",
    "    groups = df[\"datapoint\"].astype(str).to_numpy()\n",
    "\n",
    "    return df, X, y, groups\n",
    "\n",
    "\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3aadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Model\n",
    "# ---------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, HIDDEN[0]), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN[0], HIDDEN[1]), nn.ReLU(),\n",
    "            nn.Linear(HIDDEN[1], 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(1)  # logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff132c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Split by datapoint groups\n",
    "# ---------------------------\n",
    "def grouped_splits(groups: np.ndarray, val_frac=VAL_FRACTION, test_frac=TEST_FRACTION, seed=SEED):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_frac, random_state=seed)\n",
    "    all_idx = np.arange(len(groups))\n",
    "    train_val_idx, test_idx = next(gss.split(all_idx, groups=groups))\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_frac/(1-test_frac), random_state=seed)\n",
    "    train_idx, val_idx = next(gss2.split(train_val_idx, groups=groups[train_val_idx]))\n",
    "    train_idx = train_val_idx[train_idx]\n",
    "    val_idx = train_val_idx[val_idx]\n",
    "    return train_idx, val_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3b7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Metrics / thresholds\n",
    "# ---------------------------\n",
    "def eval_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else float(\"nan\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p,r,f1,_ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def choose_ternary_thresholds(y_true, y_prob, target_precision=0.9):\n",
    "    \"\"\"\n",
    "    Pick low/high cutoffs: below low -> \"bad\", above high -> \"good\", otherwise \"uncertain\".\n",
    "    We pick the smallest threshold achieving the target precision for each class on val.\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_prob)\n",
    "    probs_sorted = y_prob[order]\n",
    "    y_sorted = y_true[order]\n",
    "\n",
    "    # Low threshold for \"bad\": look at left tail (predict bad if prob < t_low)\n",
    "    best_low = 0.2  # fallback\n",
    "    for t in np.linspace(0.05, 0.5, 20):\n",
    "        pred_bad = (y_prob < t).astype(int)  # 1 for bad prediction claim\n",
    "        # precision for \"bad\" = TP_bad / (TP_bad + FP_bad) where true bad is y=0\n",
    "        tp = ((pred_bad==1) & (y_true==0)).sum()\n",
    "        fp = ((pred_bad==1) & (y_true==1)).sum()\n",
    "        prec_bad = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "        if prec_bad >= target_precision:\n",
    "            best_low = t\n",
    "            break\n",
    "\n",
    "    # High threshold for \"good\": look at right tail (predict good if prob >= t_high)\n",
    "    best_high = 0.8  # fallback\n",
    "    for t in np.linspace(0.5, 0.95, 20):\n",
    "        pred_good = (y_prob >= t).astype(int)  # 1 for good claim\n",
    "        tp = ((pred_good==1) & (y_true==1)).sum()\n",
    "        fp = ((pred_good==1) & (y_true==0)).sum()\n",
    "        prec_good = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "        if prec_good >= target_precision:\n",
    "            best_high = t\n",
    "            break\n",
    "\n",
    "    return float(best_low), float(best_high)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef68f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "def train():\n",
    "    set_seed()\n",
    "    df, X, y, groups = load_and_prepare()\n",
    "\n",
    "    # Grouped splits\n",
    "    tr_idx, va_idx, te_idx = grouped_splits(groups)\n",
    "\n",
    "    # Preprocessing on TRAIN only\n",
    "    scaler = StandardScaler().fit(X[tr_idx])\n",
    "    X_tr = scaler.transform(X[tr_idx]).astype(np.float32)\n",
    "    X_va = scaler.transform(X[va_idx]).astype(np.float32)\n",
    "    X_te = scaler.transform(X[te_idx]).astype(np.float32)\n",
    "\n",
    "    y_tr, y_va, y_te = y[tr_idx], y[va_idx], y[te_idx]\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(RNADataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader   = DataLoader(RNADataset(X_va, y_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(RNADataset(X_te, y_te), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = MLP(in_dim=X.shape[1]).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val = -np.inf\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item() * len(xb)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            yv = []; pv = []\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                pv.append(prob); yv.append(yb.numpy())\n",
    "            pv = np.concatenate(pv); yv = np.concatenate(yv)\n",
    "\n",
    "        val_auc = roc_auc_score(yv, pv) if len(np.unique(yv))>1 else float(\"nan\")\n",
    "\n",
    "        if np.isfinite(val_auc) and val_auc > best_val:\n",
    "            best_val = val_auc\n",
    "            best_state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"val_auc\": best_val\n",
    "            }\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={running/len(train_loader.dataset):.4f} | val_auc={val_auc:.4f}\")\n",
    "\n",
    "    # Load best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"state_dict\"])\n",
    "\n",
    "    # Final validation metrics and thresholds\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pv = []\n",
    "        for xb, _ in val_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            pv.append(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "        pv = np.concatenate(pv)\n",
    "\n",
    "    t_low, t_high = choose_ternary_thresholds(y_va, pv, target_precision=0.9)\n",
    "\n",
    "    # Test metrics\n",
    "    with torch.no_grad():\n",
    "        pt = []\n",
    "        yt = []\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            pt.append(torch.sigmoid(model(xb)).cpu().numpy()); yt.append(yb.numpy())\n",
    "        pt = np.concatenate(pt); yt = np.concatenate(yt)\n",
    "\n",
    "    metrics = {\n",
    "        \"val\": eval_metrics(y_va, pv, threshold=0.5),\n",
    "        \"test\": eval_metrics(yt, pt, threshold=0.5),\n",
    "        \"val_thresholds\": {\"low\": t_low, \"high\": t_high, \"target_precision\": 0.9},\n",
    "        \"split_sizes\": {\"train\": len(tr_idx), \"val\": len(va_idx), \"test\": len(te_idx)},\n",
    "    }\n",
    "    print(\"Metrics:\", json.dumps(metrics, indent=2))\n",
    "\n",
    "    # Save bundle\n",
    "    bundle = {\n",
    "        \"meta\": {\n",
    "            \"created\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"features\": FEATURES,\n",
    "            \"device_trained\": DEVICE,\n",
    "            \"seed\": SEED,\n",
    "        },\n",
    "        \"scaler\": scaler,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_class\": \"MLP\",\n",
    "        \"model_kwargs\": {\"in_dim\": X.shape[1]},\n",
    "        \"thresholds\": {\"low\": t_low, \"high\": t_high},\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    joblib.dump(bundle, BUNDLE_PATH)\n",
    "    print(f\"Saved bundle to {BUNDLE_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e5ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Inference utilities\n",
    "# ---------------------------\n",
    "def load_bundle(path=BUNDLE_PATH):\n",
    "    b = joblib.load(path)\n",
    "    model = MLP(**b[\"model_kwargs\"])\n",
    "    model.load_state_dict(b[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return b, model\n",
    "\n",
    "def predict_quality(attrs: Dict[str, float], bundle_path=BUNDLE_PATH) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    attrs: dict mapping each feature name to its numeric value.\n",
    "    Returns: prob_good (float), decision (str)\n",
    "    \"\"\"\n",
    "    b, model = load_bundle(bundle_path)\n",
    "    feats = b[\"meta\"][\"features\"]\n",
    "    scaler: StandardScaler = b[\"scaler\"]\n",
    "    t_low = b[\"thresholds\"][\"low\"]; t_high = b[\"thresholds\"][\"high\"]\n",
    "\n",
    "    # align & impute with training means if missing\n",
    "    row = pd.Series(attrs, dtype=float).reindex(feats)\n",
    "    if row.isna().any():\n",
    "        means = pd.Series(scaler.mean_, index=feats)\n",
    "        row = row.fillna(means)\n",
    "\n",
    "    X_new = scaler.transform(pd.DataFrame([row.values], columns=feats)).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        prob = torch.sigmoid(model(torch.from_numpy(X_new))).numpy().ravel()[0].item()\n",
    "\n",
    "    if prob >= t_high:\n",
    "        decision = \"probably a good prediction\"\n",
    "    elif prob < t_low:\n",
    "        decision = \"probably a bad prediction\"\n",
    "    else:\n",
    "        decision = \"cannot determine\"\n",
    "\n",
    "    return {\"prob_good\": float(prob), \"decision\": decision, \"thresholds\": {\"low\": t_low, \"high\": t_high}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09be91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.3148 | val_auc=0.9801\n",
      "Epoch 002 | train_loss=0.1724 | val_auc=0.9883\n",
      "Epoch 003 | train_loss=0.1440 | val_auc=0.9911\n",
      "Epoch 004 | train_loss=0.1264 | val_auc=0.9923\n",
      "Epoch 005 | train_loss=0.1133 | val_auc=0.9933\n",
      "Epoch 006 | train_loss=0.1030 | val_auc=0.9934\n",
      "Epoch 007 | train_loss=0.0950 | val_auc=0.9936\n",
      "Epoch 008 | train_loss=0.0881 | val_auc=0.9936\n",
      "Epoch 009 | train_loss=0.0818 | val_auc=0.9946\n",
      "Epoch 010 | train_loss=0.0761 | val_auc=0.9942\n",
      "Epoch 011 | train_loss=0.0720 | val_auc=0.9942\n",
      "Epoch 012 | train_loss=0.0686 | val_auc=0.9946\n",
      "Epoch 013 | train_loss=0.0652 | val_auc=0.9949\n",
      "Epoch 014 | train_loss=0.0618 | val_auc=0.9949\n",
      "Epoch 015 | train_loss=0.0602 | val_auc=0.9944\n",
      "Epoch 016 | train_loss=0.0565 | val_auc=0.9955\n",
      "Epoch 017 | train_loss=0.0547 | val_auc=0.9948\n",
      "Epoch 018 | train_loss=0.0519 | val_auc=0.9951\n",
      "Epoch 019 | train_loss=0.0506 | val_auc=0.9952\n",
      "Epoch 020 | train_loss=0.0485 | val_auc=0.9955\n",
      "Epoch 021 | train_loss=0.0466 | val_auc=0.9952\n",
      "Epoch 022 | train_loss=0.0453 | val_auc=0.9957\n",
      "Epoch 023 | train_loss=0.0439 | val_auc=0.9957\n",
      "Epoch 024 | train_loss=0.0416 | val_auc=0.9957\n",
      "Epoch 025 | train_loss=0.0413 | val_auc=0.9956\n",
      "Epoch 026 | train_loss=0.0396 | val_auc=0.9958\n",
      "Epoch 027 | train_loss=0.0394 | val_auc=0.9960\n",
      "Epoch 028 | train_loss=0.0371 | val_auc=0.9955\n",
      "Epoch 029 | train_loss=0.0362 | val_auc=0.9958\n",
      "Epoch 030 | train_loss=0.0374 | val_auc=0.9961\n",
      "Metrics: {\n",
      "  \"val\": {\n",
      "    \"auc\": 0.9961315550872804,\n",
      "    \"acc\": 0.9651756694010215,\n",
      "    \"precision\": 0.988020365378856,\n",
      "    \"recall\": 0.9469001148105626,\n",
      "    \"f1\": 0.9670233035321706\n",
      "  },\n",
      "  \"test\": {\n",
      "    \"auc\": 0.9979754612637042,\n",
      "    \"acc\": 0.9797245008512614,\n",
      "    \"precision\": 0.9873962496157394,\n",
      "    \"recall\": 0.9727437916414294,\n",
      "    \"f1\": 0.9800152555301297\n",
      "  },\n",
      "  \"val_thresholds\": {\n",
      "    \"low\": 0.05,\n",
      "    \"high\": 0.5,\n",
      "    \"target_precision\": 0.9\n",
      "  },\n",
      "  \"split_sizes\": {\n",
      "    \"train\": 30147,\n",
      "    \"val\": 6461,\n",
      "    \"test\": 6461\n",
      "  }\n",
      "}\n",
      "Saved bundle to quality_model_bundle.joblib\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed1be3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prob_good': 0.0, 'decision': 'probably a bad prediction', 'thresholds': {'low': 0.05, 'high': 0.5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# trying it out\n",
    "\n",
    "attrs = {\n",
    "    \"sequence_length\": 87,\n",
    "    \"gc_content\": 0.52,\n",
    "    \"sequence_entropy\": 1.83,\n",
    "    \"mfe\": -23.4,\n",
    "    \"ens_def\": 0.12,\n",
    "    \"longest_sequential_A\": 4,\n",
    "    \"longest_sequential_C\": 5,\n",
    "    \"longest_sequential_U\": 3,\n",
    "    \"longest_sequential_G\": 6,\n",
    "    \"longest_GC_helix\": 7,\n",
    "    \"GU_pairs\": 2,\n",
    "    \"rate_of_bps_predicted\": 0.38,\n",
    "    \"hairpin_count\": 3,\n",
    "    \"junction_count\": 1,\n",
    "    \"helix_count\": 5,\n",
    "    \"singlestrand_count\": 29,\n",
    "    \"mway_junction_count\": 0,\n",
    "    \"AU_pairs_in_helix_terminal_ends\": 1,\n",
    "    \"helices_with_reverse_complement\": 0,\n",
    "    \"hairpins_with_gt4_unpaired_nts\": 1,\n",
    "}\n",
    "\n",
    "print(predict_quality(attrs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc4f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prob_good': 0.9319013953208923, 'decision': 'probably a good prediction', 'thresholds': {'low': 0.05, 'high': 0.5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Known easy datapoint for testing:\n",
    "\n",
    "attrs = {\n",
    "    \"sequence_length\": 107,\n",
    "    \"gc_content\": 0.411214953271028,\n",
    "    \"sequence_entropy\": 0.9538820686999452,\n",
    "    \"mfe\": -36.9,\n",
    "    \"ens_def\": 1.67,\n",
    "    \"longest_sequential_A\": 6,\n",
    "    \"longest_sequential_C\": 2,\n",
    "    \"longest_sequential_U\": 2,\n",
    "    \"longest_sequential_G\": 3,\n",
    "    \"longest_GC_helix\": 1,\n",
    "    \"GU_pairs\": 1,\n",
    "    \"rate_of_bps_predicted\": 0.48598130841121495,\n",
    "    \"hairpin_count\": 2,\n",
    "    \"junction_count\": 2,\n",
    "    \"helix_count\": 4,\n",
    "    \"singlestrand_count\": 3,\n",
    "    \"mway_junction_count\": 0,\n",
    "    \"AU_pairs_in_helix_terminal_ends\": 0.5,\n",
    "    \"helices_with_reverse_complement\": 0.75,\n",
    "    \"hairpins_with_gt4_unpaired_nts\": 1.0,\n",
    "}\n",
    "\n",
    "print(predict_quality(attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05892018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
