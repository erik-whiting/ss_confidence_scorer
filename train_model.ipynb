{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc680ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872b063b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>datapoint</th>\n",
       "      <th>sequence</th>\n",
       "      <th>model</th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>gc_content</th>\n",
       "      <th>sequence_entropy</th>\n",
       "      <th>mfe</th>\n",
       "      <th>...</th>\n",
       "      <th>rate_of_bps_predicted</th>\n",
       "      <th>hairpin_count</th>\n",
       "      <th>junction_count</th>\n",
       "      <th>helix_count</th>\n",
       "      <th>singlestrand_count</th>\n",
       "      <th>mway_junction_count</th>\n",
       "      <th>AU_pairs_in_helix_terminal_ends</th>\n",
       "      <th>helices_with_reverse_complement</th>\n",
       "      <th>hairpins_with_gt4_unpaired_nts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EternaData</td>\n",
       "      <td>ETERNA_R00_0000_ANNOTATION_1540</td>\n",
       "      <td>GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...</td>\n",
       "      <td>ContextFold</td>\n",
       "      <td>..........(((((((....((((((...((((((....))))))...</td>\n",
       "      <td>0.953595</td>\n",
       "      <td>107</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>-36.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EternaData</td>\n",
       "      <td>ETERNA_R00_0000_ANNOTATION_1540</td>\n",
       "      <td>GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...</td>\n",
       "      <td>ContraFold</td>\n",
       "      <td>..........((((((((((.((((((.(.((((((....))))))...</td>\n",
       "      <td>0.960087</td>\n",
       "      <td>107</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>-36.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560748</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EternaData</td>\n",
       "      <td>ETERNA_R00_0000_ANNOTATION_1540</td>\n",
       "      <td>GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...</td>\n",
       "      <td>EternaFold</td>\n",
       "      <td>..........((((((((((.((((((.(.((((((....))))))...</td>\n",
       "      <td>0.960087</td>\n",
       "      <td>107</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>-36.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560748</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EternaData</td>\n",
       "      <td>ETERNA_R00_0000_ANNOTATION_1540</td>\n",
       "      <td>GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...</td>\n",
       "      <td>IPKnot</td>\n",
       "      <td>..........((((((((...((((((...((((((....))))))...</td>\n",
       "      <td>0.963431</td>\n",
       "      <td>107</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>-36.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EternaData</td>\n",
       "      <td>ETERNA_R00_0000_ANNOTATION_1540</td>\n",
       "      <td>GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...</td>\n",
       "      <td>MXFold</td>\n",
       "      <td>(.........((((((((...((((((...((((((....))))))...</td>\n",
       "      <td>0.948925</td>\n",
       "      <td>107</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.953882</td>\n",
       "      <td>-36.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset                        datapoint  \\\n",
       "0  EternaData  ETERNA_R00_0000_ANNOTATION_1540   \n",
       "1  EternaData  ETERNA_R00_0000_ANNOTATION_1540   \n",
       "2  EternaData  ETERNA_R00_0000_ANNOTATION_1540   \n",
       "3  EternaData  ETERNA_R00_0000_ANNOTATION_1540   \n",
       "4  EternaData  ETERNA_R00_0000_ANNOTATION_1540   \n",
       "\n",
       "                                            sequence        model  \\\n",
       "0  GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...  ContextFold   \n",
       "1  GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...   ContraFold   \n",
       "2  GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...   EternaFold   \n",
       "3  GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...       IPKnot   \n",
       "4  GGAAAAAAGGGUUGAUACGAUCGCUUGAUCCUGAAGGAAGCUUCAG...       MXFold   \n",
       "\n",
       "                                          prediction     score  \\\n",
       "0  ..........(((((((....((((((...((((((....))))))...  0.953595   \n",
       "1  ..........((((((((((.((((((.(.((((((....))))))...  0.960087   \n",
       "2  ..........((((((((((.((((((.(.((((((....))))))...  0.960087   \n",
       "3  ..........((((((((...((((((...((((((....))))))...  0.963431   \n",
       "4  (.........((((((((...((((((...((((((....))))))...  0.948925   \n",
       "\n",
       "   sequence_length  gc_content  sequence_entropy   mfe  ...  \\\n",
       "0              107    0.411215          0.953882 -36.9  ...   \n",
       "1              107    0.411215          0.953882 -36.9  ...   \n",
       "2              107    0.411215          0.953882 -36.9  ...   \n",
       "3              107    0.411215          0.953882 -36.9  ...   \n",
       "4              107    0.411215          0.953882 -36.9  ...   \n",
       "\n",
       "   rate_of_bps_predicted  hairpin_count  junction_count  helix_count  \\\n",
       "0               0.485981              2               2            4   \n",
       "1               0.560748              2               4            6   \n",
       "2               0.560748              2               4            6   \n",
       "3               0.504673              2               2            4   \n",
       "4               0.523364              2               3            5   \n",
       "\n",
       "   singlestrand_count  mway_junction_count  AU_pairs_in_helix_terminal_ends  \\\n",
       "0                   3                    0                             0.50   \n",
       "1                   3                    0                             0.50   \n",
       "2                   3                    0                             0.50   \n",
       "3                   3                    0                             0.25   \n",
       "4                   0                    1                             0.20   \n",
       "\n",
       "   helices_with_reverse_complement  hairpins_with_gt4_unpaired_nts  label  \n",
       "0                         0.750000                             1.0      1  \n",
       "1                         0.833333                             1.0      1  \n",
       "2                         0.833333                             1.0      1  \n",
       "3                         0.750000                             1.0      1  \n",
       "4                         0.800000                             1.0      1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_path = \"chem_map_all_easy_preds_enriched.csv\"\n",
    "hard_path = \"chem_map_all_hard_preds_enriched.csv\"\n",
    "\n",
    "easy_df = pd.read_csv(easy_path)\n",
    "hard_df = pd.read_csv(hard_path)\n",
    "\n",
    "easy_df[\"label\"] = 1\n",
    "hard_df[\"label\"] = 0\n",
    "\n",
    "df = pd.concat([easy_df, hard_df], ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584c0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    # \"score\", # Not sure I want this\n",
    "    \"sequence_length\",\n",
    "    \"gc_content\",\n",
    "    \"sequence_entropy\",\n",
    "    \"mfe\",\n",
    "    \"ens_def\",\n",
    "    \"longest_sequential_A\",\n",
    "    \"longest_sequential_C\",\n",
    "    \"longest_sequential_U\",\n",
    "    \"longest_sequential_G\",\n",
    "    \"longest_GC_helix\",\n",
    "    \"GU_pairs\",\n",
    "    \"rate_of_bps_predicted\",\n",
    "    \"hairpin_count\",\n",
    "    \"junction_count\",\n",
    "    \"helix_count\",\n",
    "    \"singlestrand_count\",\n",
    "    \"mway_junction_count\",\n",
    "    \"AU_pairs_in_helix_terminal_ends\",\n",
    "    \"helices_with_reverse_complement\",\n",
    "    \"hairpins_with_gt4_unpaired_nts\",\n",
    "]\n",
    "\n",
    "for fc in feature_cols:\n",
    "    if fc not in df.columns:\n",
    "        print(f\"{fc} not in feature columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca75f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are 0 or 1\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "assert set(df[\"label\"].unique()) <= {0, 1}, \"Labels must be 0/1 only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f243deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN in X? False\n",
      "Any inf in X? False\n"
     ]
    }
   ],
   "source": [
    "const_cols = []\n",
    "for col in feature_cols:\n",
    "    if df[col].nunique() <= 1:\n",
    "        const_cols.append(col)\n",
    "\n",
    "if const_cols:\n",
    "    print(\"Dropping constant feature columns:\", const_cols)\n",
    "    feature_cols = [c for c in feature_cols if c not in const_cols]\n",
    "\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y = df[\"label\"].values.astype(np.int64)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Any NaN in X?\", np.isnan(X).any())\n",
    "print(\"Any inf in X?\", np.isinf(X).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673a07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e692de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN in X_train_scaled? False\n",
      "Any inf in X_train_scaled? False\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Double-check after scaling\n",
    "print(\"Any NaN in X_train_scaled?\", np.isnan(X_train_scaled).any())\n",
    "print(\"Any inf in X_train_scaled?\", np.isinf(X_train_scaled).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015fc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class RNAPredictionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = RNAPredictionDataset(X_train_scaled, y_train)\n",
    "val_dataset   = RNAPredictionDataset(X_val_scaled, y_val)\n",
    "test_dataset  = RNAPredictionDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33681b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityClassifier(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),   # single logit\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QualityClassifier(in_features=len(feature_cols)).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f130496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, model, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device).float()  # must be float for BCEWithLogitsLoss\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * X_batch.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).long()\n",
    "        total_correct += (preds == y_batch.long()).sum().item()\n",
    "        total_examples += X_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    avg_acc  = total_correct / total_examples\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89be0178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss: 0.2581, train acc: 0.901 | val loss: 0.1674, val acc: 0.933\n",
      "Epoch 02 | train loss: 0.1480, train acc: 0.946 | val loss: 0.1353, val acc: 0.949\n",
      "Epoch 03 | train loss: 0.1247, train acc: 0.954 | val loss: 0.1218, val acc: 0.953\n",
      "Epoch 04 | train loss: 0.1077, train acc: 0.961 | val loss: 0.1078, val acc: 0.961\n",
      "Epoch 05 | train loss: 0.0965, train acc: 0.966 | val loss: 0.0998, val acc: 0.966\n",
      "Epoch 06 | train loss: 0.0862, train acc: 0.970 | val loss: 0.0906, val acc: 0.969\n",
      "Epoch 07 | train loss: 0.0785, train acc: 0.973 | val loss: 0.0839, val acc: 0.972\n",
      "Epoch 08 | train loss: 0.0719, train acc: 0.974 | val loss: 0.0804, val acc: 0.972\n",
      "Epoch 09 | train loss: 0.0667, train acc: 0.976 | val loss: 0.0749, val acc: 0.976\n",
      "Epoch 10 | train loss: 0.0623, train acc: 0.978 | val loss: 0.0688, val acc: 0.977\n",
      "Epoch 11 | train loss: 0.0580, train acc: 0.980 | val loss: 0.0672, val acc: 0.979\n",
      "Epoch 12 | train loss: 0.0552, train acc: 0.980 | val loss: 0.0635, val acc: 0.979\n",
      "Epoch 13 | train loss: 0.0511, train acc: 0.982 | val loss: 0.0629, val acc: 0.981\n",
      "Epoch 14 | train loss: 0.0489, train acc: 0.983 | val loss: 0.0585, val acc: 0.982\n",
      "Epoch 15 | train loss: 0.0466, train acc: 0.985 | val loss: 0.0587, val acc: 0.982\n",
      "Epoch 16 | train loss: 0.0439, train acc: 0.985 | val loss: 0.0532, val acc: 0.982\n",
      "Epoch 17 | train loss: 0.0434, train acc: 0.986 | val loss: 0.0502, val acc: 0.985\n",
      "Epoch 18 | train loss: 0.0413, train acc: 0.986 | val loss: 0.0532, val acc: 0.982\n",
      "Epoch 19 | train loss: 0.0391, train acc: 0.987 | val loss: 0.0486, val acc: 0.985\n",
      "Epoch 20 | train loss: 0.0371, train acc: 0.988 | val loss: 0.0500, val acc: 0.984\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, model, optimizer)\n",
    "    val_loss, val_acc     = run_epoch(val_loader, model, optimizer=None)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f}, train acc: {train_acc:.3f} | \"\n",
    "        f\"val loss: {val_loss:.4f}, val acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8bb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0401, Test acc: 0.985\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = run_epoch(test_loader, model, optimizer=None)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cffb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a3aae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QualityClassifier.__init__() missing 1 required positional argument: 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      7\u001b[39m net = NeuralNetRegressor(\n\u001b[32m      8\u001b[39m     module=QualityClassifier,   \u001b[38;5;66;03m# your nn.Module class (not instance)\u001b[39;00m\n\u001b[32m      9\u001b[39m     max_epochs=\u001b[32m50\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m pipe = Pipeline([\n\u001b[32m     16\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, StandardScaler()),\n\u001b[32m     17\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mpca\u001b[39m\u001b[33m\"\u001b[39m, PCA(n_components=\u001b[32m2\u001b[39m)),\n\u001b[32m     18\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m, net),\n\u001b[32m     19\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     23\u001b[39m joblib.dump({\u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: pipe, \u001b[33m\"\u001b[39m\u001b[33mfeature_cols\u001b[39m\u001b[33m\"\u001b[39m: feature_cols}, \u001b[33m\"\u001b[39m\u001b[33mmodel_bundle.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\regressor.py:85\u001b[39m, in \u001b[36mNeuralNetRegressor.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[33;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetRegressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\net.py:1335\u001b[39m, in \u001b[36mNeuralNet.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m   1303\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize and fit the module.\u001b[39;00m\n\u001b[32m   1304\u001b[39m \n\u001b[32m   1305\u001b[39m \u001b[33;03mIf the module was already initialized, by calling fit, the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1332\u001b[39m \n\u001b[32m   1333\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initialized_:\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28mself\u001b[39m.partial_fit(X, y, **fit_params)\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\net.py:921\u001b[39m, in \u001b[36mNeuralNet.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    919\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_virtual_params()\n\u001b[32m    920\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_callbacks()\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_criterion()\n\u001b[32m    923\u001b[39m \u001b[38;5;28mself\u001b[39m._initialize_optimizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\net.py:780\u001b[39m, in \u001b[36mNeuralNet._initialize_module\u001b[39m\u001b[34m(self, reason)\u001b[39m\n\u001b[32m    777\u001b[39m             msg = \u001b[38;5;28mself\u001b[39m._format_reinit_msg(\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m, kwargs)\n\u001b[32m    778\u001b[39m         \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialize_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;66;03m# deal with device\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\net.py:627\u001b[39m, in \u001b[36mNeuralNet.initialize_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initializes the module.\u001b[39;00m\n\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03mIf the module is already initialized and no parameter was changed, it\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03mwill be left as is.\u001b[39;00m\n\u001b[32m    624\u001b[39m \n\u001b[32m    625\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    626\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m.get_params_for(\u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minitialized_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28mself\u001b[39m.module_ = module\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eedee\\miniconda3\\Lib\\site-packages\\skorch\\net.py:604\u001b[39m, in \u001b[36mNeuralNet.initialized_instance\u001b[39m\u001b[34m(self, instance_or_cls, kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_init:\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(instance_or_cls)(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance_or_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: QualityClassifier.__init__() missing 1 required positional argument: 'in_features'"
     ]
    }
   ],
   "source": [
    "# pip install skorch\n",
    "from skorch import NeuralNetRegressor   # or NeuralNetClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=QualityClassifier,   # your nn.Module class (not instance)\n",
    "    max_epochs=50,\n",
    "    lr=1e-3,\n",
    "    batch_size=64,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2)),\n",
    "    (\"estimator\", net),\n",
    "])\n",
    "pipe.fit(X, y)\n",
    "\n",
    "import joblib\n",
    "joblib.dump({\"pipeline\": pipe, \"feature_cols\": feature_cols}, \"model_bundle.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90866c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
